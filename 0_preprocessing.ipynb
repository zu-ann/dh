{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 ms, sys: 4.27 ms, total: 32.8 ms\n",
      "Wall time: 44.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "names = ['filename', 'title', 'year', 'author', 'years_of_life',\n",
    "         'time_summary', 'time_book', 'name', 'username', 'tradition_country']\n",
    "df = pd.read_csv('metatable.tsv', sep='\\t', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 477 µs, total: 11.1 ms\n",
      "Wall time: 11.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "split_tradition_country = df['tradition_country'].str.split('->')\n",
    "df['tradition']= split_tradition_country.str.get(0).apply(lambda x: x.replace('\\xad', ''))\n",
    "df['country']=split_tradition_country.str.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 0 ns, total: 11.1 ms\n",
      "Wall time: 10.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['title'] = df['title'].str.replace('\\xa0', ' ')\n",
    "df['title'] = df['title'].str.split('<').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.4 ms, sys: 7.54 ms, total: 34.9 ms\n",
      "Wall time: 42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "split_years_of_life = df['years_of_life'].str.split('–')\n",
    "df['year_of_birth'] = pd.to_numeric(split_years_of_life.str.get(0), errors='coerce')\n",
    "df['year_of_death'] = pd.to_numeric(split_years_of_life.str.get(1), errors='coerce')\n",
    "df['epoch'] = df[['year_of_birth', 'year_of_death']].mean(axis=1) // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_letters = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "capital_letters_quotes = capital_letters + '«»''\"\"“”  '\n",
    "\n",
    "def separate_heading(text, capital_letters):\n",
    "    heading = list(text[:200])\n",
    "    for i, letter in enumerate(heading):\n",
    "        if letter in capital_letters and i-1 > 0 and heading[i-1] not in capital_letters_quotes:\n",
    "            heading[i] = ' ' + letter\n",
    "    return ''.join(heading) + text[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.25 s, sys: 736 ms, total: 3.99 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['text'] = ''\n",
    "for i in df.index:\n",
    "    with open(df.loc[i, 'filename'], 'r', encoding='utf-8') as f:\n",
    "        text = f.read().replace('\\xad', ' ').replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "        text = separate_heading(text , capital_letters)\n",
    "        df.loc[i, 'text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            tokens.append(word.lower().translate(TABLE))\n",
    "    tokens = list(filter(lambda a: a not in '  ', tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('russian') + ['свой', 'который', 'весь', 'это']\n",
    "\n",
    "punctuation = set(punctuation + '«»—–…“”\\n\\t' + digits)\n",
    "TABLE = str.maketrans({ch: ' ' for ch in punctuation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokenized'] = df.apply(lambda r: tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'COM': 'ADJ', 'APRO': 'DET', 'PART': 'PART', 'PR': 'ADP', 'ADV': 'ADV', 'INTJ': 'INTJ',\n",
    "           'S': 'NOUN', 'V': 'VERB', 'CONJ': 'SCONJ', 'UNKN': 'X', 'ANUM': 'ADJ', 'NUM': 'NUM',\n",
    "           'NONLEX': 'X', 'SPRO': 'PRON', 'ADVPRO': 'ADV', 'A': 'ADJ'}\n",
    "pymystem = Mystem()\n",
    "\n",
    "def pymystem_lemmatize_text(pymystem, text, mapping, del_stopwords=True):\n",
    "    lemmas = []\n",
    "    lemmas_pos = []\n",
    "    ana = pymystem.analyze(text.translate(TABLE))\n",
    "    \n",
    "    for word in ana:\n",
    "        if word.get('analysis') and len(word.get('analysis')) > 0:\n",
    "            lemma = word['analysis'][0]['lex'].lower().strip()\n",
    "            \n",
    "            if del_stopwords:\n",
    "                if lemma in STOPWORDS:\n",
    "                    continue\n",
    "                    \n",
    "            lemmas.append(lemma)\n",
    "                \n",
    "            pos = word['analysis'][0]['gr'].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if pos in mapping:\n",
    "                lemmas_pos.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                lemmas_pos.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге  \n",
    "    \n",
    "    return lemmas, lemmas_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.6 s, sys: 1.7 s, total: 57.3 s\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = df.apply(lambda r: pymystem_lemmatize_text(pymystem, r['text'], mapping), axis=1).values\n",
    "df_res = pd.DataFrame(list(res), columns = ['text_pymystem_list', 'text_pymystem_pos_list'])\n",
    "df = df.join(df_res)\n",
    "df.to_pickle('metatable_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# res = df.apply(lambda r: pymystem_lemmatize_text(pymystem, r['text'], mapping, del_stopwords=False), axis=1).values\n",
    "# df_res = pd.DataFrame(list(res), columns = ['text_pymystem_list_with_stopwords',\n",
    "#                                             'text_pymystem_pos_list_with_stopwords'])\n",
    "# df = df.join(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_to_save = df.drop(['text_pymystem_list_with_stopwords', 'text_pymystem_pos_list_with_stopwords'], 1)\n",
    "# df_to_save.to_pickle('metatable_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_dataframe(df, col_to_groupby, first_arg, *args):\n",
    "    grouped = df.groupby([col_to_groupby])\n",
    "    \n",
    "    def func(data):\n",
    "        if isinstance(data.iloc[0], str):\n",
    "            return ' '.join\n",
    "        elif isinstance(data.iloc[0], list):\n",
    "            return sum\n",
    "    \n",
    "    df_res = grouped[first_arg].agg(func(df[first_arg])).reset_index()\n",
    "    for arg in args:\n",
    "        df_res[arg] = grouped[arg].agg(func(df[arg])).reset_index()[arg]\n",
    "        \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.5 s, sys: 1.01 s, total: 45.5 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traditions = get_grouped_dataframe(df, 'tradition',\n",
    "                                   'text_tokenized',\n",
    "                                   'text_pymystem_list',\n",
    "                                   'text_pymystem_pos_list')\n",
    "traditions.to_pickle('traditions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 923 ms, total: 18.2 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "countries = get_grouped_dataframe(df, 'country',\n",
    "                                  'text_tokenized', \n",
    "                                  'text_pymystem_list',\n",
    "                                  'text_pymystem_pos_list')\n",
    "countries.to_pickle('countries.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 965 ms, sys: 228 ms, total: 1.19 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traditions_topic_modeling = df.groupby(['tradition'])['text_pymystem_list'].apply(list).reset_index()\n",
    "traditions_topic_modeling.to_pickle('traditions_topic_modeling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 868 ms, sys: 197 ms, total: 1.07 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traditions_topic_modeling = df.groupby(['country'])['text_pymystem_list'].apply(list).reset_index()\n",
    "traditions_topic_modeling.to_pickle('countries_topic_modeling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 688 ms, sys: 4.55 ms, total: 693 ms\n",
      "Wall time: 753 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grouped_by_authors = df.groupby(['author']).count()\n",
    "needed_authors_list = grouped_by_authors.iloc[np.where(grouped_by_authors['text'] > 5)].index\n",
    "needed_authors_grouped = df[df['author'].isin(needed_authors_list)].groupby(['author'])\n",
    "needed_authors = needed_authors_grouped['text_pymystem_list'].apply(list).reset_index()\n",
    "\n",
    "needed_authors.to_pickle('authors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_for_stylo(df, column_name, column_text, n_tokens, dir_name):\n",
    "    grouped = df.groupby([column_name])\n",
    "    needed_list = grouped.count().iloc[np.where(grouped[column_text].agg(sum).agg(len) >= n_tokens * 2)].index\n",
    "    needed_grouped = df[df[column_name].isin(needed_list)].groupby([column_name])\n",
    "    df_res = needed_grouped[column_text].apply(sum).reset_index()\n",
    "    \n",
    "    for user_id in df_res.index:\n",
    "        l = df_res.loc[user_id, column_text]\n",
    "        lst = [df_res.loc[user_id, column_text][i : i + n_tokens] for i in range(0, len(l) - (len(l) % n_tokens),\n",
    "                                                                             n_tokens)]\n",
    "        for data_id, data in enumerate(lst):\n",
    "            print('{}: {} whole_len:{} part:{} len:{}'.format(column_name, df_res.loc[user_id, column_name],\n",
    "                                                    needed_grouped[column_text].apply(sum).apply(len).iloc[user_id],\n",
    "                                                    data_id, len(data)))\n",
    "            with open(os.getcwd() + '/corpus/{}/{}_part{}.txt'.format(dir_name, \n",
    "                                                                      df_res.loc[user_id, column_name],\n",
    "                                                                      data_id + 1), 'w', encoding='utf-8') as f:\n",
    "                 f.write(' '.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: 123 whole_len:21786 part:0 len:5000\n",
      "username: 123 whole_len:21786 part:1 len:5000\n",
      "username: 123 whole_len:21786 part:2 len:5000\n",
      "username: 123 whole_len:21786 part:3 len:5000\n",
      "username: 152 whole_len:12128 part:0 len:5000\n",
      "username: 152 whole_len:12128 part:1 len:5000\n",
      "username: 165 whole_len:10630 part:0 len:5000\n",
      "username: 165 whole_len:10630 part:1 len:5000\n",
      "username: 226 whole_len:18177 part:0 len:5000\n",
      "username: 226 whole_len:18177 part:1 len:5000\n",
      "username: 226 whole_len:18177 part:2 len:5000\n",
      "username: 227 whole_len:13709 part:0 len:5000\n",
      "username: 227 whole_len:13709 part:1 len:5000\n",
      "username: 233 whole_len:13579 part:0 len:5000\n",
      "username: 233 whole_len:13579 part:1 len:5000\n",
      "username: belov whole_len:27915 part:0 len:5000\n",
      "username: belov whole_len:27915 part:1 len:5000\n",
      "username: belov whole_len:27915 part:2 len:5000\n",
      "username: belov whole_len:27915 part:3 len:5000\n",
      "username: belov whole_len:27915 part:4 len:5000\n",
      "username: erkhov whole_len:24845 part:0 len:5000\n",
      "username: erkhov whole_len:24845 part:1 len:5000\n",
      "username: erkhov whole_len:24845 part:2 len:5000\n",
      "username: erkhov whole_len:24845 part:3 len:5000\n",
      "username: gasparov whole_len:45144 part:0 len:5000\n",
      "username: gasparov whole_len:45144 part:1 len:5000\n",
      "username: gasparov whole_len:45144 part:2 len:5000\n",
      "username: gasparov whole_len:45144 part:3 len:5000\n",
      "username: gasparov whole_len:45144 part:4 len:5000\n",
      "username: gasparov whole_len:45144 part:5 len:5000\n",
      "username: gasparov whole_len:45144 part:6 len:5000\n",
      "username: gasparov whole_len:45144 part:7 len:5000\n",
      "username: gasparov whole_len:45144 part:8 len:5000\n",
      "username: karelsky whole_len:46381 part:0 len:5000\n",
      "username: karelsky whole_len:46381 part:1 len:5000\n",
      "username: karelsky whole_len:46381 part:2 len:5000\n",
      "username: karelsky whole_len:46381 part:3 len:5000\n",
      "username: karelsky whole_len:46381 part:4 len:5000\n",
      "username: karelsky whole_len:46381 part:5 len:5000\n",
      "username: karelsky whole_len:46381 part:6 len:5000\n",
      "username: karelsky whole_len:46381 part:7 len:5000\n",
      "username: karelsky whole_len:46381 part:8 len:5000\n",
      "username: kostyrko whole_len:14261 part:0 len:5000\n",
      "username: kostyrko whole_len:14261 part:1 len:5000\n",
      "username: rynkevich whole_len:19532 part:0 len:5000\n",
      "username: rynkevich whole_len:19532 part:1 len:5000\n",
      "username: rynkevich whole_len:19532 part:2 len:5000\n",
      "username: sanovich whole_len:23980 part:0 len:5000\n",
      "username: sanovich whole_len:23980 part:1 len:5000\n",
      "username: sanovich whole_len:23980 part:2 len:5000\n",
      "username: sanovich whole_len:23980 part:3 len:5000\n",
      "username: shishkin whole_len:27638 part:0 len:5000\n",
      "username: shishkin whole_len:27638 part:1 len:5000\n",
      "username: shishkin whole_len:27638 part:2 len:5000\n",
      "username: shishkin whole_len:27638 part:3 len:5000\n",
      "username: shishkin whole_len:27638 part:4 len:5000\n",
      "username: smirnov whole_len:24079 part:0 len:5000\n",
      "username: smirnov whole_len:24079 part:1 len:5000\n",
      "username: smirnov whole_len:24079 part:2 len:5000\n",
      "username: smirnov whole_len:24079 part:3 len:5000\n",
      "username: zhidkov whole_len:11941 part:0 len:5000\n",
      "username: zhidkov whole_len:11941 part:1 len:5000\n",
      "username: zhivotovsky whole_len:11583 part:0 len:5000\n",
      "username: zhivotovsky whole_len:11583 part:1 len:5000\n"
     ]
    }
   ],
   "source": [
    "create_files_for_stylo(df, 'username', 'text_tokenized', 5000, 'username_tokenized_5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: 123 whole_len:21786 part:0 len:10000\n",
      "username: 123 whole_len:21786 part:1 len:10000\n",
      "username: belov whole_len:27915 part:0 len:10000\n",
      "username: belov whole_len:27915 part:1 len:10000\n",
      "username: erkhov whole_len:24845 part:0 len:10000\n",
      "username: erkhov whole_len:24845 part:1 len:10000\n",
      "username: gasparov whole_len:45144 part:0 len:10000\n",
      "username: gasparov whole_len:45144 part:1 len:10000\n",
      "username: gasparov whole_len:45144 part:2 len:10000\n",
      "username: gasparov whole_len:45144 part:3 len:10000\n",
      "username: karelsky whole_len:46381 part:0 len:10000\n",
      "username: karelsky whole_len:46381 part:1 len:10000\n",
      "username: karelsky whole_len:46381 part:2 len:10000\n",
      "username: karelsky whole_len:46381 part:3 len:10000\n",
      "username: sanovich whole_len:23980 part:0 len:10000\n",
      "username: sanovich whole_len:23980 part:1 len:10000\n",
      "username: shishkin whole_len:27638 part:0 len:10000\n",
      "username: shishkin whole_len:27638 part:1 len:10000\n",
      "username: smirnov whole_len:24079 part:0 len:10000\n",
      "username: smirnov whole_len:24079 part:1 len:10000\n"
     ]
    }
   ],
   "source": [
    "create_files_for_stylo(df, 'username', 'text_tokenized', 10000, 'username_tokenized_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: 123 whole_len:21948 part:0 len:5000\n",
      "username: 123 whole_len:21948 part:1 len:5000\n",
      "username: 123 whole_len:21948 part:2 len:5000\n",
      "username: 123 whole_len:21948 part:3 len:5000\n",
      "username: 152 whole_len:12218 part:0 len:5000\n",
      "username: 152 whole_len:12218 part:1 len:5000\n",
      "username: 165 whole_len:10618 part:0 len:5000\n",
      "username: 165 whole_len:10618 part:1 len:5000\n",
      "username: 226 whole_len:18170 part:0 len:5000\n",
      "username: 226 whole_len:18170 part:1 len:5000\n",
      "username: 226 whole_len:18170 part:2 len:5000\n",
      "username: 227 whole_len:13718 part:0 len:5000\n",
      "username: 227 whole_len:13718 part:1 len:5000\n",
      "username: 233 whole_len:13689 part:0 len:5000\n",
      "username: 233 whole_len:13689 part:1 len:5000\n",
      "username: belov whole_len:28119 part:0 len:5000\n",
      "username: belov whole_len:28119 part:1 len:5000\n",
      "username: belov whole_len:28119 part:2 len:5000\n",
      "username: belov whole_len:28119 part:3 len:5000\n",
      "username: belov whole_len:28119 part:4 len:5000\n",
      "username: erkhov whole_len:24937 part:0 len:5000\n",
      "username: erkhov whole_len:24937 part:1 len:5000\n",
      "username: erkhov whole_len:24937 part:2 len:5000\n",
      "username: erkhov whole_len:24937 part:3 len:5000\n",
      "username: gasparov whole_len:45426 part:0 len:5000\n",
      "username: gasparov whole_len:45426 part:1 len:5000\n",
      "username: gasparov whole_len:45426 part:2 len:5000\n",
      "username: gasparov whole_len:45426 part:3 len:5000\n",
      "username: gasparov whole_len:45426 part:4 len:5000\n",
      "username: gasparov whole_len:45426 part:5 len:5000\n",
      "username: gasparov whole_len:45426 part:6 len:5000\n",
      "username: gasparov whole_len:45426 part:7 len:5000\n",
      "username: gasparov whole_len:45426 part:8 len:5000\n",
      "username: karelsky whole_len:46776 part:0 len:5000\n",
      "username: karelsky whole_len:46776 part:1 len:5000\n",
      "username: karelsky whole_len:46776 part:2 len:5000\n",
      "username: karelsky whole_len:46776 part:3 len:5000\n",
      "username: karelsky whole_len:46776 part:4 len:5000\n",
      "username: karelsky whole_len:46776 part:5 len:5000\n",
      "username: karelsky whole_len:46776 part:6 len:5000\n",
      "username: karelsky whole_len:46776 part:7 len:5000\n",
      "username: karelsky whole_len:46776 part:8 len:5000\n",
      "username: kostyrko whole_len:14342 part:0 len:5000\n",
      "username: kostyrko whole_len:14342 part:1 len:5000\n",
      "username: rynkevich whole_len:19624 part:0 len:5000\n",
      "username: rynkevich whole_len:19624 part:1 len:5000\n",
      "username: rynkevich whole_len:19624 part:2 len:5000\n",
      "username: sanovich whole_len:24143 part:0 len:5000\n",
      "username: sanovich whole_len:24143 part:1 len:5000\n",
      "username: sanovich whole_len:24143 part:2 len:5000\n",
      "username: sanovich whole_len:24143 part:3 len:5000\n",
      "username: shishkin whole_len:27750 part:0 len:5000\n",
      "username: shishkin whole_len:27750 part:1 len:5000\n",
      "username: shishkin whole_len:27750 part:2 len:5000\n",
      "username: shishkin whole_len:27750 part:3 len:5000\n",
      "username: shishkin whole_len:27750 part:4 len:5000\n",
      "username: smirnov whole_len:24170 part:0 len:5000\n",
      "username: smirnov whole_len:24170 part:1 len:5000\n",
      "username: smirnov whole_len:24170 part:2 len:5000\n",
      "username: smirnov whole_len:24170 part:3 len:5000\n",
      "username: zhidkov whole_len:11996 part:0 len:5000\n",
      "username: zhidkov whole_len:11996 part:1 len:5000\n",
      "username: zhivotovsky whole_len:11668 part:0 len:5000\n",
      "username: zhivotovsky whole_len:11668 part:1 len:5000\n"
     ]
    }
   ],
   "source": [
    "create_files_for_stylo(df, 'username', 'text_pymystem_list_with_stopwords', 5000, 'username_lemmatized_5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: 123 whole_len:21948 part:0 len:10000\n",
      "username: 123 whole_len:21948 part:1 len:10000\n",
      "username: belov whole_len:28119 part:0 len:10000\n",
      "username: belov whole_len:28119 part:1 len:10000\n",
      "username: erkhov whole_len:24937 part:0 len:10000\n",
      "username: erkhov whole_len:24937 part:1 len:10000\n",
      "username: gasparov whole_len:45426 part:0 len:10000\n",
      "username: gasparov whole_len:45426 part:1 len:10000\n",
      "username: gasparov whole_len:45426 part:2 len:10000\n",
      "username: gasparov whole_len:45426 part:3 len:10000\n",
      "username: karelsky whole_len:46776 part:0 len:10000\n",
      "username: karelsky whole_len:46776 part:1 len:10000\n",
      "username: karelsky whole_len:46776 part:2 len:10000\n",
      "username: karelsky whole_len:46776 part:3 len:10000\n",
      "username: sanovich whole_len:24143 part:0 len:10000\n",
      "username: sanovich whole_len:24143 part:1 len:10000\n",
      "username: shishkin whole_len:27750 part:0 len:10000\n",
      "username: shishkin whole_len:27750 part:1 len:10000\n",
      "username: smirnov whole_len:24170 part:0 len:10000\n",
      "username: smirnov whole_len:24170 part:1 len:10000\n"
     ]
    }
   ],
   "source": [
    "create_files_for_stylo(df, 'username', 'text_pymystem_list_with_stopwords', 10000, 'username_lemmatized_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Сайкаку    10190\n",
       "Name: text_tokenized, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name = 'author'\n",
    "column_text = 'text_tokenized'\n",
    "n_tokens = 5000\n",
    "\n",
    "grouped = df.groupby([column_name])\n",
    "needed_list = grouped.count().iloc[np.where(grouped[column_text].agg(sum).agg(len) >= n_tokens * 2)].index\n",
    "needed_grouped = df[df[column_name].isin(needed_list)].groupby([column_name])\n",
    "needed_grouped[column_text].apply(sum).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Сайкаку    10335\n",
       "Name: text_pymystem_list_with_stopwords, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name = 'author'\n",
    "column_text = 'text_pymystem_list_with_stopwords'\n",
    "n_tokens = 5000\n",
    "\n",
    "grouped = df.groupby([column_name])\n",
    "needed_list = grouped.count().iloc[np.where(grouped[column_text].agg(sum).agg(len) >= n_tokens * 2)].index\n",
    "needed_grouped = df[df[column_name].isin(needed_list)].groupby([column_name])\n",
    "needed_grouped[column_text].apply(sum).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author: Сайкаку whole_len:10190 part:0 len:5000\n",
      "author: Сайкаку whole_len:10190 part:1 len:5000\n"
     ]
    }
   ],
   "source": [
    "create_files_for_stylo(df, 'author', 'text_tokenized', 5000, 'author_tokenized_5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author: Сайкаку whole_len:10335 part:0 len:5000\n",
      "author: Сайкаку whole_len:10335 part:1 len:5000\n"
     ]
    }
   ],
   "source": [
    "create_files_for_stylo(df, 'author', 'text_pymystem_list_with_stopwords', 5000, 'author_lemmatized_5000')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
